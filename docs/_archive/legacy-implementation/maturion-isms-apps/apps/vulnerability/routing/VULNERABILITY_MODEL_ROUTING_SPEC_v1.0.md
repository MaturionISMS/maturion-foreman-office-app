Below is the complete VULNERABILITY_MODEL_ROUTING_SPEC_v1.0.md, aligned with:

Vulnerability True North v1.0

Threat Model Routing Spec v1.0 (parent module)

VULNERABILITY_AI_INTERACTION_POINTS defined in:

Evidence engine

TVRE

Classification

Severity suggestion

UE NLG

Watchdog logic (AI validation pipeline)

Zero Regression & Safe AI Doctrine

This specification defines exactly which AI models are used where, when, how, and under what guardrails.
It is the authoritative routing map used by Foreman and the AI Orchestrator.

Place in:
/Modules/Risk Management/VulnerabilityModule/VULNERABILITY_MODEL_ROUTING_SPEC_v1.0.md

VULNERABILITY_MODEL_ROUTING_SPEC_v1.0.md

AI Model Routing Specification — Vulnerability Module
Version: 1.0
Maintainer: Maturion AI Orchestrator
Purpose: Define all AI calls, routing rules, fallback behaviour, context packaging, safety checks and constraints for the Vulnerability Module.

0. INTRODUCTION

The Vulnerability Module uses AI extensively, but following strict SRMF rules:

AI assists

Human approves

System logs

No direct DB writes

All AI output must be explainable

This document defines:

AI entry points

Model selections

Context windows

Fallback models

Safety flows

Output validation

Versioning of prompts

Forbidden operations

It ensures deterministic behaviour across the SRMF pipeline.

1. AI ENTRY POINTS (MAP)

AI is used in the following 6 operations:

1. Vulnerability Classification (category/subcategory)
2. Severity Suggestion
3. Description Polish (NLP enhancement)
4. Evidence Interpretation (Vision Model)
5. TVRE Refinement (Threat-Vulnerability Relevance)
6. Unwanted Event (UE) Sentence Generation (NLG)


Each section below contains routing specifications.

2. MODEL CATEGORIES USED

The following model groups are referenced:

2.1 NLP-STRUCTURED

Used for:

metadata classification

severity suggestion

description improvement

structured reasoning

2.2 NLP-NARRATIVE

Used for:

UE sentence generation

Natural language refinement

Summaries

2.3 VISION

Used for:

evidence interpretation

markup detection

anomaly detection

2.4 EMBEDDING

Used for:

similarity mapping

architecture-to-threat mapping context

evidence clustering

2.5 HYBRID (NLP + VISION)

Used for:

combined evidence + vulnerability reasoning

contextual anomaly scoring

3. CONFIGURABLE MODEL TIERS

To accommodate performance vs cost strategies:

Tier A: High-power (default for staging & prod)

Large multimodal models with full context

High accuracy

Tier B: Mid-power (dev)

Smaller models for code tests and validation

Tier C: Offline fallback

Local embeddings + deterministic rules

For safety scenarios

The system automatically switches tiers using Model Routing Rules.

4. MODEL ROUTING DECISION LOGIC

Model selection is based on:

task type

input size

context complexity

performance budget

accuracy requirements

privacy rules (vision content)

Routing follows this decision tree:

If vision input → Vision model
Else if structured classification → NLP-structured
Else if narrative generation → NLP-narrative
Else if reasoning across modules → Hybrid
Else → fallback deterministic logic

5. AI CALL SPECIFICATIONS (DETAILED)
5.1 Classification Engine (Category/Subcategory)
Purpose:

Assist users in classifying vulnerabilities consistently.

Routing:
Model Type: NLP-STRUCTURED  
Tier: A → B → C fallback
Context:  
- vulnerability.title  
- vulnerability.description  
- architecture.node_metadata  
- threat summaries (compressed)
Output: JSON classification proposal
Requires: User approval

Output Contract:
{
  "category": "...",
  "subcategory": "...",
  "confidence": 0-1
}

Safety:

Must provide 3 alternative classifications

Must explain classification reasoning

Confidence < 0.3 triggers Watchdog

5.2 Severity Suggestion Engine
Purpose:

Provide recommended severity level.

Routing:
Model Type: NLP-STRUCTURED
Tier: A or B
Context:
- exploitability score
- classification
- threat relevance (if available)
- architecture criticality
Output: severity suggestion

Output Contract:
{
  "severity_level": "Low|Medium|High|Critical",
  "justification": "...",
  "confidence": 0-1
}

Notes:

System never auto-assigns severity

Custodian must approve

5.3 Description Polish Engine
Purpose:

Improve grammar, clarity, and formal tone.

Routing:
Model: NLP-NARRATIVE (Tier A)
Context:
- original description
- classification
- architecture path
- evidence tags (if available)

Output:

Polished paragraph

No new facts allowed

No invented details

Guardrails:

Compare semantic diff between input/output

If >10% deviation → reject

5.4 Evidence Interpretation (Vision)
Purpose:

Detect likely vulnerabilities, anomalies, or hotspots.

Routing:
Model Type: VISION (Tier A)
Context:
- image or PDF
- vulnerability scope
- location metadata

Output Contract:
{
  "objects_detected": [...],
  "potential_issues": [...],
  "markup_suggestions": [...],
  "confidence": 0-1
}

Safety:

Vision model outputs never override human markups

Watchdog logs low-confidence results

5.5 TVRE Refinement Engine
Purpose:

Assist TVRE by analysing:

threat TTPs

vulnerability exploitability

architecture exposure

real evidence

Routing:
Model Type: HYBRID (NLP + Vision) or NLP-Structured only
Context:
- threat profile
- vulnerability metadata
- evidence metadata
- drift score
- architecture context

Output:
{
  "relevance_score": 0-1,
  "valid": true/false,
  "ttp_matches": [...],
  "justification": "..."
}

Safety:

AI cannot change the final relevance score

It only recommends

TVRE remains deterministic

5.6 UE (Unwanted Event) Sentence Generator
Purpose:

Generate clear, structured, NIST-aligned UEs.

Routing:
Model: NLP-NARRATIVE (Tier A)
Context:
- vulnerability description
- threat description
- architecture location
- exploitability score

Output:
{
  "sentence": "A [actor] exploiting [vulnerability] at [location] may cause [unwanted event]...",
  "confidence": 0-1
}

Safety:

Must not change threat type

Must not exaggerate severity

Must not create fictitious actors or assets

Human must approve

6. FALLBACK LOGIC

If AI is offline or fails safety checks:

6.1 Classification fallback

→ deterministic taxonomy rules based on keywords.

6.2 Severity fallback

→ base severity = exploitability × architecture criticality.

6.3 UE fallback

→ template-based UE generator.

6.4 TVRE fallback

→ baseline scoring formula only.

6.5 Evidence fallback

→ no automatic interpretation.

7. WATCHDOG INTEGRATION

Watchdog monitors:

low confidence AI outputs

hallucination indicators

improbable classifications

empty or malformed responses

deviation from expected structure

Alerts raised:

Issue	Severity
AI confidence <0.3	Medium
AI classification mismatch >40%	High
UE coherence score <0.8	High
Vision bounding boxes inconsistent	Medium
8. AI LOGGING REQUIREMENTS

Every AI output must be written to:

vulnerability_ai_interaction_log

Fields:

vuln_id

ai_task_type

input_hash

output_hash

model_used

confidence

user_accepted

For audit and reproducibility.

9. MODEL PROMPT VERSIONING

All prompts used in this module must be versioned:

/ai_prompts/vulnerability/
    classification_prompt_v1.0.txt
    severity_prompt_v1.0.txt
    description_polish_v1.0.txt
    ue_generation_prompt_v1.0.txt
    tvre_refinement_prompt_v1.0.txt
    evidence_analysis_prompt_v1.0.txt


Changing a prompt requires:

Foreman approval

Version increment

Watchdog regression test

10. PERMISSIONS & SAFETY RULES

AI is forbidden from:

making final approvals

modifying severity

altering evidence

committing DB changes

bypassing workflow

injecting new threats

creating UEs with incorrect actors or assets

overriding deterministic TVRE output

Human remains final authority.

11. EXAMPLE REQUEST/RESPONSE PAIRS
11.1 Classification

Input:
“Loose door latch at warehouse receiving entrance.”

Output:

category: Physical Security
subcategory: Door Integrity
confidence: 0.88

11.2 UE Generation

Input: Threat + Vulnerability → UE

Output:
“A criminal opportunist exploiting a loose door latch at the receiving entrance may gain unauthorized access to the warehouse.”

12. ACCEPTANCE CRITERIA (v1.0)

Routing spec complete when:

All AI entry points defined

All model categories mapped

All routing logic defined

All safety guardrails defined

All fallback paths defined

All integration points documented

Log schema valid

Foreman signs off

✔ END OF VULNERABILITY_MODEL_ROUTING_SPEC_v1.0.md